{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# package_path = '../input/pytorch-image-models/pytorch-image-models-master' #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n",
    "# sys.path.append(package_path)\n",
    "\n",
    "# import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "# sys.path.append('../input/pytorchimagemodels')\n",
    "\n",
    "# efficientnet_path='../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master'\n",
    "\n",
    "# sys.path.append(efficientnet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_path='../input/efficientnet-pytorch10/EfficientNet-PyTorch-1.0'\n",
    "# ../input/efficientnet-pytorch10/EfficientNet-PyTorch-1.0\n",
    "sys.path.append(efficientnet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "from functools import partial\n",
    "\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import cv2\n",
    "import pydicom\n",
    "import timm #from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf ns effnet and ViT\n",
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'n_classes': 5,\n",
    "    'seed': 719,\n",
    "    'merged_data':True,\n",
    "    'evaluate': True,\n",
    "    'submit': True,\n",
    "    'multi_model': True,\n",
    "    'model_arch': 'tf_efficientnet_b3_ns', # 'seresnext101_32x4d', #'tf_efficientnet_b3_ns', #'efficientnet-b3', #'tf_efficientnet_b3_ns',\n",
    "    'model_arch2': 'vit_base_patch16_384', # vit_base_patch16_384 deit_base_patch16_384 'seresnext101_32x4d', #'tf_efficientnet_b3_ns', #'efficientnet-b3', #'tf_efficientnet_b3_ns',\n",
    "    'model_arch3': 'deit_base_patch16_384',\n",
    "    'model_arch4': 'resnext50_32x4d', \n",
    "    'model_arch5': 'seresnext101_32x4d', \n",
    "    'img_size': 512, #512, #256,\n",
    "    'img_size2': 384,\n",
    "    'img_size3': 384,\n",
    "    'img_size4': 512,\n",
    "    'epochs': 50,\n",
    "    'train_bs': 8, #64,\n",
    "    'valid_bs': 32,\n",
    "    'T_0': 10,\n",
    "    'lr': 1e-5,\n",
    "    'min_lr': 1e-6,\n",
    "    'weight_decay':1e-6,\n",
    "    'num_workers': 4,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'weight_dirs': ['../input/cassava-efficientnet/best_score_effnet_b3_ls03_v2', \n",
    "                    '../input/cassava-efficientnet/best_score_resnext50_data_v2', \n",
    "                    '../input/cassava-efficientnet/best_score_vit_p16_ls03_data_v2',\n",
    "                    ], #'../input/cassava-efficientnet/best_score_vit_p16_ls03','../input/cassava-efficientnet/best_score_tf_ns_ls_5folds'# best_score_seresnext101_data_v2 best_score_resnext50_data_v2'' '../input/cassava-efficientnet/best_score_vit_p16_ls03'], # best_score_tf_ns_ls_5folds'], #best_score_seresnext101_ls03_agb4', ../input/cassava-efficientnet/best_score_tf_ns_ls_5folds best_score_vit_p32_ls03\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 7, # best 6\n",
    "    'weights': [1,1,1,1,1,1,1,1],\n",
    "    'fold_weights': [1,1,1,1,1] #[1,1,1,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deit\n",
    "# CFG = {\n",
    "#     'fold_num': 5,\n",
    "#     'n_classes': 5,\n",
    "#     'seed': 719,\n",
    "#     'merged_data':True,\n",
    "#     'evaluate': True,\n",
    "#     'multi_model': False,\n",
    "#     'model_arch': 'deit_base_patch16_384', # 'seresnext101_32x4d', #'tf_efficientnet_b3_ns', #'efficientnet-b3', #'tf_efficientnet_b3_ns',\n",
    "#     'model_arch2': 'deit_base_patch16_384', # vit_base_patch16_384 'seresnext101_32x4d', #'tf_efficientnet_b3_ns', #'efficientnet-b3', #'tf_efficientnet_b3_ns',\n",
    "#     'img_size': 384, #512, #256,\n",
    "#     'img_size2': 384,\n",
    "#     'epochs': 50,\n",
    "#     'train_bs': 8, #64,\n",
    "#     'valid_bs': 32,\n",
    "#     'T_0': 10,\n",
    "#     'lr': 1e-5,\n",
    "#     'min_lr': 1e-6,\n",
    "#     'weight_decay':1e-6,\n",
    "#     'num_workers': 4,\n",
    "#     'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "#     'weight_dirs': ['../input/cassava-efficientnet/best_score_deit_p16_ls03_data_v2_ep10'], #best_score_deit_p16_ls03'], best_score_deit_p16_ls03 best_score_tf_ns_ls_5folds, '../input/cassava-efficientnet/best_score_vit_p16_ls03'], #['../input/cassava-efficientnet/best_score_tf_ns_bitempered02','../input/cassava-efficientnet/best_score_tf_ns_ls_5folds'], #best_score_seresnext101_ls03_agb4', ../input/cassava-efficientnet/best_score_tf_ns_ls_5folds best_score_vit_p32_ls03\n",
    "#     'verbose_step': 1,\n",
    "#     'device': 'cuda:0',\n",
    "#     'tta': 4,\n",
    "#     'weights': [1,1,1,1,1],\n",
    "#     'fold_weights': [1,1,1,1,1] #[1,1,1,1]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # seresnext101_32x4d\n",
    "# CFG = {\n",
    "#     'fold_num': 5,\n",
    "#     'n_classes': 5,\n",
    "#     'seed': 719,\n",
    "#     'merged_data':True,\n",
    "#     'evaluate': True,\n",
    "#     'multi_model': False,\n",
    "#     'model_arch': 'resnext50_32x4d', # 'seresnext101_32x4d', #'tf_efficientnet_b3_ns', #'efficientnet-b3', #'tf_efficientnet_b3_ns',\n",
    "#     'model_arch2': 'vit_base_patch16_384', # vit_base_patch16_384 deit_base_patch16_384 'seresnext101_32x4d', #'tf_efficientnet_b3_ns', #'efficientnet-b3', #'tf_efficientnet_b3_ns',\n",
    "#     'model_arch3': 'deit_base_patch16_384',\n",
    "#     'model_arch4': 'resnext50_32x4d', \n",
    "#     'img_size': 512, #512, #256,\n",
    "#     'img_size2': 384,\n",
    "#     'img_size3': 384,\n",
    "#     'img_size4': 512,\n",
    "#     'vit_img_size': 384,\n",
    "#     'epochs': 50,\n",
    "#     'train_bs': 8, #64,\n",
    "#     'valid_bs': 32,\n",
    "#     'T_0': 10,\n",
    "#     'lr': 1e-5,\n",
    "#     'min_lr': 1e-6,\n",
    "#     'weight_decay':1e-6,\n",
    "#     'num_workers': 4,\n",
    "#     'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "#     'weight_dirs': ['../input/cassava-efficientnet/best_score_resnext50_data_v2'], #best_score_seresnext101_ls03_agb4',\n",
    "#     'verbose_step': 1,\n",
    "#     'device': 'cuda:0',\n",
    "#     'tta': 4,\n",
    "#     'weights': [1,1,1,1,1],\n",
    "#     'fold_weights': [1,1,1,1,1] #[1,1,1,1]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vit\n",
    "# CFG = {\n",
    "#     'fold_num': 5,\n",
    "#     'n_classes': 5,\n",
    "#     'seed': 719,\n",
    "#     'merged_data':True,\n",
    "#     'evaluate': True,\n",
    "#     'multi_model': False,\n",
    "#     'model_arch': 'vit_base_patch16_384', # 'seresnext101_32x4d', #'tf_efficientnet_b3_ns', #'efficientnet-b3', #'tf_efficientnet_b3_ns',\n",
    "#     'model_arch2': None, # 'seresnext101_32x4d', #'tf_efficientnet_b3_ns', #'efficientnet-b3', #'tf_efficientnet_b3_ns',\n",
    "#     'img_size': 384, #512, #256,\n",
    "#     'img_size2': 384,\n",
    "#     'epochs': 50,\n",
    "#     'train_bs': 32, #64,\n",
    "#     'valid_bs': 32,\n",
    "#     'T_0': 10,\n",
    "#     'lr': 1e-5,\n",
    "#     'min_lr': 1e-6,\n",
    "#     'weight_decay':1e-6,\n",
    "#     'num_workers': 4,\n",
    "#     'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "#     'weight_dirs': ['../input/cassava-efficientnet/best_score_vit_p16_ls03_data_v2_ep10'], # ../input/cassava-efficientnet/best_score_vit_p16_ls03_data_v2'], #best_score_vit_p32_ls03'],\n",
    "#     'verbose_step': 1,\n",
    "#     'device': 'cuda:0',\n",
    "#     'tta': 4,\n",
    "#     'weights': [1,1,1,1,1],\n",
    "#     'fold_weights': [1,1,1,1,1] #[1,1,1,1]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We could do stratified validation split in each fold to make each fold's train and validation set looks like the whole train set in target distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    #print(im_rgb)\n",
    "    return im_rgb\n",
    "\n",
    "# img = get_img('../input/cassava-leaf-disease-classification/train_images/1000015157.jpg')\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df, data_root, transforms=None, output_label=True\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.df.iloc[index]['label']\n",
    "          \n",
    "        path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n",
    "        \n",
    "        img  = get_img(path)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "            \n",
    "        # do label smoothing\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train\\Validation Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms(img_size=CFG['img_size']):\n",
    "    return Compose([\n",
    "            CenterCrop(img_size, img_size, p=1.),\n",
    "            Resize(img_size, img_size),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "def get_inference_transforms(img_size=CFG['img_size']):\n",
    "    return Compose([\n",
    "            RandomResizedCrop(img_size, img_size),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "# def get_inference_transforms(img_size=CFG['img_size']):\n",
    "#     return Compose([\n",
    "#             RandomResizedCrop(img_size, img_size),\n",
    "#             Transpose(p=0.5),\n",
    "#             HorizontalFlip(p=0.5),\n",
    "#             VerticalFlip(p=0.5),\n",
    "#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "#             RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "#             ToTensorV2(p=1.0),\n",
    "#         ], p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "# @register_model\n",
    "# def deit_base_patch16_224(pretrained=False, **kwargs):\n",
    "#     model = VisionTransformer(\n",
    "#         patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     model.default_cfg = _cfg()\n",
    "#     if pretrained:\n",
    "#         checkpoint = torch.hub.load_state_dict_from_url(\n",
    "#             url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
    "#             map_location=\"cpu\", check_hash=True\n",
    "#         )\n",
    "#         model.load_state_dict(checkpoint[\"model\"])\n",
    "#     return model\n",
    "\n",
    "# self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "#                  num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None,\n",
    "#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0., hybrid_backbone=None, norm_layer=None):\n",
    "\n",
    "@register_model\n",
    "def deit_base_patch16_384(pretrained=False, **kwargs):\n",
    "    \"\"\" DeiT base model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).\n",
    "    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n",
    "    \"\"\"\n",
    "#     model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)\n",
    "#     model = _create_vision_transformer('vit_deit_base_patch16_384', pretrained=pretrained, **model_kwargs)\n",
    "#     model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_arch, n_class, pretrained=False):\n",
    "    if 'tf_' in model_arch and 'ns' in model_arch:\n",
    "        print('Loading EfficientNet model')\n",
    "        model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(n_features, n_class)\n",
    "    elif 'resnext' in model_arch:\n",
    "        print('Loading ResNext model')\n",
    "        model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "#         n_features = model.last_linear.in_features\n",
    "#         model.last_linear = nn.Linear(n_features, n_class)\n",
    "        n_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(n_features, n_class)\n",
    "\n",
    "    elif 'vit' in model_arch:\n",
    "        print('Loading ViT model')\n",
    "        model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        model.head = nn.Linear(model.head.in_features, n_class)\n",
    "    elif 'deit' in model_arch:\n",
    "        print('Loading DeiT model')\n",
    "#         model = torch.hub.load('facebookresearch/deit:main', model_arch, pretrained=pretrained)\n",
    "        model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = model.head.in_features\n",
    "        model.head = nn.Linear(n_features, n_class)\n",
    "    else:\n",
    "        model = EfficientNet.from_name(model_arch)\n",
    "        if pretrained:\n",
    "            model.load_state_dict(torch.load(pretrained))\n",
    "    #         self.efficient_net=EfficientNet.from_pretrained('efficientnet-b3',num_classes=CLASSES)\n",
    "        in_features=model._fc.in_features\n",
    "        model._fc=nn.Linear(in_features,n_class)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        \n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_valid(train, folds, weight_dirs, class_col_name='label'):\n",
    "    device = torch.device(CFG['device'])\n",
    "    val_preds_all = 0\n",
    "    val_preds_all_2 = 0\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "#         if fold > 0:\n",
    "#             break\n",
    "        print('Validation Inference fold {} started'.format(fold))\n",
    "   \n",
    "        val_preds = []\n",
    "       #TODO: uncomment for loop\n",
    "        for i, weight_dir in enumerate(weight_dirs): \n",
    "            if 'best' in weight_dir or 'basemodel' in weight_dir:\n",
    "                weight_path = glob(f'{weight_dir}/fold{fold}*')\n",
    "    #             weight_path = glob(f'{weight_dir}/best_val_score_fold{fold}*')\n",
    "                weight_path = weight_path[0]\n",
    "            else:\n",
    "                weight_path = f'{weight_dir}/best_{mode}_{CFG[\"model_arch\"]}_fold_{fold}.pth'\n",
    "            \n",
    "            if multi_model and 'vit' in weight_path:\n",
    "                img_size = CFG['img_size2']\n",
    "                model_arch = CFG['model_arch2']\n",
    "            elif multi_model and 'deit' in weight_path:\n",
    "                img_size = CFG['img_size3']\n",
    "                model_arch = CFG['model_arch3']\n",
    "            elif multi_model and 'seresnext101' in weight_path:\n",
    "                img_size = CFG['img_size4']\n",
    "                model_arch = CFG['model_arch5']\n",
    "            elif multi_model and 'resnext50' in weight_path:\n",
    "                img_size = CFG['img_size4']\n",
    "                model_arch = CFG['model_arch4']\n",
    "            else:\n",
    "                img_size = CFG['img_size']\n",
    "                model_arch = CFG['model_arch']\n",
    "                \n",
    "            print(f'Loading weight: {weight_path}')\n",
    "\n",
    "            if 'deit' in weight_path or 'seresnext' in weight_path:\n",
    "#                 weight_path = '../input/cassava-efficientnet/fold0_epoch0_val_score0.855.pt'\n",
    "                model = torch.load(weight_path)\n",
    "                model.cuda()\n",
    "            else:\n",
    "                model = get_model(model_arch, CFG['n_classes']).to(device) #CFG['pretrained_path'] n_classes train.label.nunique()\n",
    "                model.load_state_dict(torch.load(weight_path))\n",
    "                \n",
    "            if CFG['merged_data'] and 'v2' in weight_path:\n",
    "                print('Loading Merged Data.')\n",
    "                valid_ = train[train['fold'].astype(int)==fold]\n",
    "                img_path = '../input/cassava-leaf-disease-merged/train/'\n",
    "            else:\n",
    "                valid_ = train.loc[val_idx,:].reset_index(drop=True)\n",
    "                img_path = '../input/cassava-leaf-disease-classification/train_images/'\n",
    "                \n",
    "            valid_ds = CassavaDataset(valid_, img_path, \n",
    "                                      transforms=get_inference_transforms(img_size), output_label=False)\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                    valid_ds, \n",
    "                    batch_size=CFG['valid_bs'],\n",
    "                    num_workers=CFG['num_workers'],\n",
    "                    shuffle=False,\n",
    "                    pin_memory=False,\n",
    "                )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _ in range(CFG['tta']):\n",
    "                    val_preds += [CFG['weights'][0]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n",
    "        #############################\n",
    "        val_preds = np.mean(val_preds, axis=0) \n",
    "        print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_[class_col_name].values, val_preds)))\n",
    "        print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_[class_col_name].values==np.argmax(val_preds, axis=1)).mean()))\n",
    "        val_preds_all += (valid_[class_col_name].values==np.argmax(val_preds, axis=1)).mean()\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    fold_score = val_preds_all / CFG['fold_num']\n",
    "    return print(f'{CFG[\"fold_num\"]} folds mean Accuracy: {fold_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(test, folds, weight_dirs, loaded_state_dict=True):\n",
    "    device = torch.device(CFG['device'])\n",
    "    files_path = '../input/cassava-leaf-disease-classification/test_images/'\n",
    "    test_size = len(os.listdir(files_path))\n",
    "    test_preds_all = np.zeros((test_size, CFG['n_classes']))\n",
    "    tst_preds_all = 0\n",
    "    tst_preds_all_2 = 0\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        print('Inference fold {} started'.format(fold))\n",
    "\n",
    "        tst_preds = []\n",
    "        for i, weight_dir in enumerate(weight_dirs):\n",
    "\n",
    "            if 'best' in weight_dir or 'basemodel' in weight_dir:\n",
    "                weight_path = glob(f'{weight_dir}/fold{fold}*')\n",
    "                weight_path = weight_path[0]\n",
    "            else:\n",
    "                weight_path = f'{weight_dir}/best_{mode}_{CFG[\"model_arch\"]}_fold_{fold}.pth'\n",
    "                \n",
    "            if multi_model and 'vit' in weight_path:\n",
    "                img_size = CFG['img_size2']\n",
    "                model_arch = CFG['model_arch2']\n",
    "            elif multi_model and 'deit' in weight_path:\n",
    "                img_size = CFG['img_size3']\n",
    "                model_arch = CFG['model_arch3']\n",
    "            elif multi_model and 'seresnext101' in weight_path:\n",
    "                img_size = CFG['img_size4']\n",
    "                model_arch = CFG['model_arch5']\n",
    "            elif multi_model and 'resnext50' in weight_path:\n",
    "                img_size = CFG['img_size4']\n",
    "                model_arch = CFG['model_arch4']\n",
    "            else:\n",
    "                img_size = CFG['img_size']\n",
    "                model_arch = CFG['model_arch']\n",
    "    \n",
    "            if 'deit' in weight_path or 'seresnext' in weight_path:\n",
    "                model = torch.load(weight_path)\n",
    "                model.cuda()\n",
    "            else:\n",
    "                model = get_model(model_arch, CFG['n_classes']).to(device) #CFG['pretrained_path']\n",
    "                model.load_state_dict(torch.load(weight_path))\n",
    "\n",
    "            test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', \n",
    "                                     transforms=get_inference_transforms(img_size), output_label=False)\n",
    "\n",
    "            tst_loader = torch.utils.data.DataLoader(\n",
    "                test_ds, \n",
    "                batch_size=CFG['valid_bs'],\n",
    "                num_workers=CFG['num_workers'],\n",
    "                shuffle=False,\n",
    "                pin_memory=False,\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _ in range(CFG['tta']):\n",
    "                    tst_preds += [CFG['weights'][0]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n",
    "\n",
    "        tst_preds = np.mean(tst_preds, axis=0) \n",
    "        \n",
    "        tst_preds_all += tst_preds\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    tst_preds_all /= CFG['fold_num']\n",
    "    return tst_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "seed_everything(CFG['seed'])\n",
    " # for training only, need nightly build pytorch\n",
    "test = pd.DataFrame()\n",
    "test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n",
    "\n",
    "if CFG['merged_data']:\n",
    "    train = pd.read_csv('../input/cassava-2019-2020-dataset/merged_5folds.csv')\n",
    "    fold_col = 'stratify_group'\n",
    "    class_col_name = 'class_id'\n",
    "else:\n",
    "    train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')  \n",
    "    fold_col = 'label'\n",
    "    class_col_name = 'label'\n",
    "    \n",
    "folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train[fold_col].values)\n",
    "\n",
    "mode = 'acc'\n",
    "evaluate = CFG['evaluate']\n",
    "weight_dirs = CFG['weight_dirs']\n",
    "multi_model = CFG['multi_model']\n",
    "\n",
    "if evaluate:\n",
    "    predict_valid(train, folds, weight_dirs, class_col_name=class_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "# seed_everything(CFG['seed'])\n",
    " # for training only, need nightly build pytorch\n",
    "test = pd.DataFrame()\n",
    "test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n",
    "\n",
    "if CFG['merged_data']:\n",
    "    train = pd.read_csv('../input/cassava-2019-2020-dataset/merged_5folds.csv')\n",
    "    fold_col = 'stratify_group'\n",
    "    class_col_name = 'class_id'\n",
    "else:\n",
    "    train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')  \n",
    "    fold_col = 'label'\n",
    "    class_col_name = 'label'\n",
    "\n",
    "folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train[fold_col].values)\n",
    "\n",
    "mode = 'acc'\n",
    "weight_dirs = CFG['weight_dirs']\n",
    "multi_model = CFG['multi_model']\n",
    "\n",
    "submit = CFG['submit']\n",
    "\n",
    "if submit:\n",
    "    tst_preds_all =  predict_test(test, folds, weight_dirs)\n",
    "    test['label'] = np.argmax(tst_preds_all, axis=1)\n",
    "    test.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
